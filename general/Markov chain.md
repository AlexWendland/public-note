---
aliases: null
checked: false
created: 2023-12-03
last_edited: 2023-12-03
publish: true
tags:
  - maths
  - probability
type: definition
---
>[!tldr] Markov chain
>A Markov chain is specified by a number of states $N$ and a transition [[Stochastic matrix|probability matrix]] $P \in M_{N \times N}(\mathbb{R})$. Intuitively think of this as a state machine which at state $i$ has probability $p_{i,j}$ of transitioning to state $j$.

