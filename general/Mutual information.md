---
aliases: 
checked: false
created: 2024-02-24
last_edited: 2024-02-24
draft: false
tags:
  - probability
type: definition
---
>[!tldr] Mutual information
>Suppose we have two [[Random variable|random variables]] $X$ and $Y$ over different [[Function domain|domains]] $A$ and $B$. Then the *mutual information* is defined to be
> $$I(X, Y) = H(Y) - H(Y \vert X).$$
> Where $H(Y)$ is the [[Information entropy|information entropy]] of $Y$ and $H(Y \vert X)$ is [[Conditional entropy|conditional entropy]].

