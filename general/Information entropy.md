---
aliases:
  - information entropy
checked: false
created: 2024-01-11
last_edited: 2024-01-11
publish: true
tags:
  - machine-learning
  - maths
type: definition
---
>[!tldr] Information Entropy
>Suppose we have a discrete [[Random variable|random variable]] $X$ that can take values in $\{1, 2, \ldots, k\}$. We define the *information entropy* of $X$ to be
>$$Entropy(X) = - \sum_{i=1}^k \mathbb{P}(X = i) \log \left ( \mathbb{P}(X = i) \right ).$$
>The higher the entropy the more uniform the random variable.



