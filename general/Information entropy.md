---
aliases:
  - information entropy
checked: false
created: 2024-01-11
last_edited: 2024-01-11
draft: false
tags:
  - machine-learning
  - maths
type: definition
---
>[!tldr] Information Entropy
>Suppose we have a discrete [[Random variable|random variable]] $X$ that can take values in $A$. We define the *information entropy* of $X$ to be
>$$H(X) = - \sum_{a \in A} \mathbb{P}(X = a) \log_2 \left ( \mathbb{P}(X = a) \right ).$$
>The higher the entropy the more uniform the random variable.



