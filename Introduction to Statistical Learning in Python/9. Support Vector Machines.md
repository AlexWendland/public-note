---
aliases: 
checked: false
created: 2024-08-17
last_edited: 2024-08-17
publish: true
tags: 
type: lecture
---
# Support vector machines

Itcha "This method is not really used any more."

There are 3 levels:
- Maximal margin classifier: Requires linearly separable
- Support vector classifier: Requires linear decision boundary
- Support vector Machine: Requires knowledge of the decision boundary

![[Hyperplane]]

Another way to define a hyperplane is to use an equation. This is to take an orthogonal vector $o$ to the subspace spanned by $v_1, v_2, \ldots, v_n$ perform the [[Dot product]] with that vector and set it equal to the dot product of $o \cdot x$. Intuitively this is the "distance" away from the hyperplane that point is.

This means a hyperplane cuts $V \backslash P$ into two halves.
$$
H_- =\{v \in V \vert o \cdot v < 0\}, \mbox{ and } H_+ = \{v \in V \vert o \cdot v > 0 \}.
$$
This provides a nice starting point for classifiers.

## Starting to classify

![[Linearly separable]]

Suppose we have $k$ points of [[Training data|training data]] with classification labels $1, -1$. This will be represented by $(x^j, y^j)$ for $1 \leq j \leq k-1$, where $x^j \in \mathbb{R}^n$ and $y^j \in \{1, -1\}$. Lets assume the data is linearly separable so there exists $w_i, \theta \in \mathbb{R}$ such that
$$
y^j\left ( - \theta + \sum_{i=1}^n x^j_i w_i \right ) \geq 0.
$$
Though in most cases if the data is linearly separable there are infinite choices of $w_i, \theta$ where you wiggle the line around.

## Best line

The magnitude of
$$
y^j\left ( - \theta + \sum_{i=1}^n x^j_i w_i \right )
$$
relates to how far the point $x^j$ is away from the [[Hyperplane|hyperplane]] $P$. We call this the margin for that point and ideally we would want to maximise the minimum margin. This will make the least assumptions about our data and reduce the chances of [[Overfitting|overfitting]].

A [[Hyperplane|hyperplane]] that achieves this is called a *maximal margin classifier*.

*talk about support vectors and actually your limitations come from very few points that will be equidistant from the line.*

## Constructing the maximal margin classifier




## Aside (neural networks)

![[Perceptron (neural network)|perceptron]]

![[Binary step|binary step]]

