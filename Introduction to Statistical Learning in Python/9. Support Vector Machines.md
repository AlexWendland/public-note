---
aliases: 
checked: false
created: 2024-08-17
last_edited: 2024-08-17
publish: true
tags: 
type: lecture
---
# Support vector machines

Itcha "This method is not really used any more."

There are 3 levels:
- Maximal margin classifier: Requires linearly separable
- Support vector classifier: Requires linear decision boundary
- Support vector Machine: Requires knowledge of the decision boundary

![[Hyperplane]]

Another way to define a hyperplane is to use an equation. This is to take an orthogonal vector $o$ to the subspace spanned by $v_1, v_2, \ldots, v_n$ perform the [[Dot product]] with that vector and set it equal to the dot product of $o \cdot x$. Intuitively this is the "distance" away from the hyperplane that point is.

This means a hyperplane cuts $V \backslash P$ into two halves.
$$
H_- =\{v \in V \vert o \cdot v < 0\}, \mbox{ and } H_+ = \{v \in V \vert o \cdot v > 0 \}.
$$
This provides a nice starting point for classifiers.

## Starting to classify

![[Linearly separable]]

Suppose we have $k$ points of [[Training data|training data]] with classification labels $1, -1$. This will be represented by $(x^j, y^j)$ for $1 \leq j \leq k-1$, where $x^j \in \mathbb{R}^n$ and $y^j \in \{1, -1\}$. Lets assume the data is linearly separable so there exists $w_i, \theta \in \mathbb{R}$ such that
$$
y^j\left ( - \theta + \sum_{i=1}^n x^j_i w_i \right ) \geq 0.
$$
There are infinite choices of $w_i, \theta$ some of these represent the same solution. You can get rid of these by forcing
$$
(w_1, w_2, \cdots, w_n) \cdot (w_1, w_2, \ldots, w_n) = \sum_{i=1}^n w_i^2 = 1.
$$
Intuitively this forces the orthogonal vector to be unit. Though there are still two choices of vector to point out of the "top" or "bottom" though this is forced by the classification within our data.

This now means that $(\theta, w_i)$ now define a unique classification. However in *most* cases there are still infinitely many choices of $(\theta, w_i)$. 

## Best line

The magnitude of
$$
y^j\left (\theta + \sum_{i=1}^n x^j_i w_i \right )
$$
(note I replaced $-\theta$ with $\theta$ to make life easier) relates to how far the point $x^j$ is away from the [[Hyperplane|hyperplane]] $P$. We call this the margin for that point and ideally we would want to maximise the minimum margin. This will make the least assumptions about our data and reduce the chances of [[Overfitting|overfitting]].

A [[Hyperplane|hyperplane]] that achieves this is called a *maximal margin classifier*. I.e. it is looking for the max $M$ such that
$$
y^j\left (\theta + \sum_{i=1}^n x^j_i w_i \right ) \geq M \mbox{ for all } j
$$
This is an interesting requirement, as most of our data does not really matter for this. There will be key points of our data that will act as the limits on our data. These will be called *support vectors*. 

(Note these support vectors and classifier will be unique!)

## Constructing the maximal margin classifier

Lets now collect all our requirements for the maximal margin classifier. We want to choose $w_i, \theta$ such that
$$
\begin{align*}
\max_{\theta, w_1, \ldots w_n, M} \ M\\
\sum_{i=1}^n w_i^2 = 1\\
y^j (\theta + \sum_{i=1}^n x_i^jw_i) \geq M & \ \mbox{for all } j
\end{align*}
$$

This kind of problem is called [[Quadratic programming problem]] which is not touched by the book. 

## Aside (neural networks)

This is essentially the same as a neural network at this point!

![[Perceptron (neural network)|perceptron]]

![[Binary step|binary step]]

## Non-seperating case

