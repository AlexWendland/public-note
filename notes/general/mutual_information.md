---
aliases:
checked: false
created: 2024-02-24
draft: false
last_edited: 2024-02-24
tags:
  - probability
title: Mutual information
type: definition
---
>[!tldr] Mutual information
>Suppose we have two [random variables](random_variable.md) $X$ and $Y$ over different [domains](function_domain.md) $A$ and $B$. Then the *mutual information* is defined to be
> $$I(X, Y) = H(Y) - H(Y \vert X).$$
> Where $H(Y)$ is the [information entropy](information_entropy.md) of $Y$ and $H(Y \vert X)$ is [conditional entropy](conditional_entropy.md).

