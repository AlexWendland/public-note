---
aliases:
checked: false
created: 2024-02-24
draft: false
last_edited: 2024-02-24
tags:
  - machine-learning
type: definition
---
>[!tldr] Joint Entropy
>Suppose we have two [[Random variable|random variables]] $X$ and $Y$ over different [[Function domain|domains]] $A$ and $B$. The *joint entropy* is defined by
>$$H(X, Y) = - \sum_{a \in A} \sum_{b \in B} \mathbb{P}[X = a, Y = b] \log_2(\mathbb{P}[X = a, Y = b]).$$
>This is [[Information entropy]] of the [[Joint distribution|joint distribution]].

