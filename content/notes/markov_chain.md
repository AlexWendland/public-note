---
aliases:
created: 2023-12-03
date_checked:
draft: false
last_edited: 2026-02-05
tags:
  - maths
  - probability
title: Markov chain
type: definition
---
>[!definition] Markov chain
>A Markov chain is specified by a number of states $N$ and a transition [probability matrix](stochastic_matrix.md) $P \in M_{N \times N}(\mathbb{R})$. Intuitively think of this as a state machine which at state $i$ has probability $p_{i,j}$ of transitioning to state $j$.

