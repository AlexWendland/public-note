---
aliases:
  - information entropy
created: 2024-01-11
date_checked:
draft: false
last_edited: 2024-01-11
tags:
  - machine-learning
  - maths
title: Information entropy
type: definition
---
>[!tldr] Information Entropy
>Suppose we have a discrete [random variable](random_variable.md) $X$ that can take values in $A$. We define the *information entropy* of $X$ to be
>$$H(X) = - \sum_{a \in A} \mathbb{P}(X = a) \log_2 \left ( \mathbb{P}(X = a) \right ).$$
>The higher the entropy the more uniform the random variable.



